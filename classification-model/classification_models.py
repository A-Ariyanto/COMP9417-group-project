# -*- coding: utf-8 -*-
"""classification_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ddunMlDIjV7knw8ozS34pW7NqYmbqO9I

# Library Import
"""

import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

"""# Load Dataset"""

df = pd.read_csv("AirQuality_cleaned.csv", index_col="timestamp", parse_dates=True)

"""# Preparation"""

horizons = [1, 6, 12, 24]

"""# CO Discretization"""

CO_class_le = LabelEncoder()
CO_class_le.fit(["low", "mid", "high"])

def get_CO_class(CO_val):
    if CO_val < 1.5:
        return "low"
    elif CO_val < 2.5:
        return "mid"
    else:
        return "high"

def get_CO_classes(CO_vals, prefix):
    CO_class = CO_vals.apply(get_CO_class)
    CO_class_l_encoded = CO_class_le.transform(CO_class)
    CO_class_oh_encoded = pd.get_dummies(
        pd.Series(CO_class),
        prefix = prefix,
        dtype = int,
        drop_first = False
    )
    CO_classes = pd.DataFrame({
        prefix + "_class": CO_class,
        prefix + "_class_l_encoded": CO_class_l_encoded
    })

    return pd.concat([CO_classes, CO_class_oh_encoded], axis = 1)

df_temp = df.copy()
CO_classes = get_CO_classes(df_temp["CO(GT)"], "CO")
df_temp = pd.concat([df_temp, CO_classes], axis = 1)

for h in horizons:
    df_temp[f"CO(GT)_fut_{h}h"] = df_temp[f"CO(GT)_fut_{h}h"].fillna(df_temp[f"CO(GT)_fut_{h}h"].mean())
    CO_fut_h_classes = get_CO_classes(df_temp[f"CO(GT)_fut_{h}h"], f"CO_fut_{h}h")
    df_temp = pd.concat([df_temp, CO_fut_h_classes], axis = 1)

pd.set_option('display.max_columns', None)
df_temp.head(5)

"""# Data Splitting"""

train = df_temp[df_temp.index.year == 2004]
test = df_temp[df_temp.index.year == 2005]

predictors = [
    "PT08.S1(CO)",
    "PT08.S2(NMHC)",
    "PT08.S3(NOx)",
    "PT08.S4(NO2)",
    "PT08.S5(O3)",
    "T",
    "RH",
    "AH",
    "hour",
    "weekday",
    "month",
    "CO(GT)_lag_1h",
    "CO(GT)_lag_6h",
    "CO(GT)_lag_12h",
    "CO(GT)_lag_24h",
    # "CO_lag_1",
    # "CO_lag_6",
    # "CO_lag_12",
    # "CO_lag_24",
    "CO(GT)_ma_1h",
    "CO(GT)_ma_6h",
    "CO(GT)_ma_12h",
    "CO(GT)_ma_24h",
    # "CO_roll_mean_1",
    # "CO_roll_mean_6",
    # "CO_roll_mean_12",
    # "CO_roll_mean_24"
]

scaled_predictors = []

for predictor in predictors:
    scaled_predictors.append(predictor + "_scaled")

"""# Classification"""

random_state = 23

def predict_CO_class_using_decision_tree(params, train_df, test_df, h):
    classifier = DecisionTreeClassifier(
        max_depth = params["max_depth"],
        min_samples_leaf = params["min_samples_leaf"],
        max_features = params["max_features"],
        random_state = random_state
    )
    classifier.fit(train_df[scaled_predictors], train_df[f"CO_fut_{h}h_class_l_encoded"])
    return classifier.predict(test_df[scaled_predictors])

def predict_CO_class_using_support_vector_machine(params, train_df, test_df, h):
    classifier = SVC(
        kernel = "rbf",
        C = params["C"],
        gamma = params["gamma"],
        random_state = random_state
    )
    classifier.fit(train_df[scaled_predictors], train_df[f"CO_fut_{h}h_class_l_encoded"])
    return classifier.predict(test_df[scaled_predictors])

def predict_CO_class_using_logistic_regression(params, train_df, test_df, h):
    low_classifier = LogisticRegression(
        penalty = "l2",
        C = params["C"],
        solver = "lbfgs",
        max_iter = 1000,
        random_state = random_state
    )
    low_classifier.fit(train_df[scaled_predictors], train_df[f"CO_fut_{h}h_low"])

    mid_classifier = LogisticRegression(
        penalty = "l2",
        C = params["C"],
        solver = "lbfgs",
        max_iter = 1000,
        random_state = random_state
    )
    mid_classifier.fit(train_df[scaled_predictors], train_df[f"CO_fut_{h}h_mid"])

    high_classifier = LogisticRegression(
        penalty = "l2",
        C = params["C"],
        solver = "lbfgs",
        max_iter = 1000,
        random_state = random_state
    )
    high_classifier.fit(train_df[scaled_predictors], train_df[f"CO_fut_{h}h_high"])

    prob_matrix = np.zeros((len(test_df), 3))
    CO_class_l_encoded = CO_class_le.transform(["low", "mid", "high"])
    prob_matrix[:, CO_class_l_encoded[0]] = low_classifier.predict_proba(test_df[scaled_predictors])[:, 1]
    prob_matrix[:, CO_class_l_encoded[1]] = mid_classifier.predict_proba(test_df[scaled_predictors])[:, 1]
    prob_matrix[:, CO_class_l_encoded[2]] = high_classifier.predict_proba(test_df[scaled_predictors])[:, 1]

    return np.argmax(prob_matrix, axis = 1)

def predict_CO_class_using_naive_baseline(params, train_df, test_df, h):
    return test_df["CO_class_l_encoded"]

predictor_by_clf_name = {
    "Decision Tree": predict_CO_class_using_decision_tree,
    "Support Vector Machine": predict_CO_class_using_support_vector_machine,
    "Logistic Regression": predict_CO_class_using_logistic_regression,
    "Naive Baseline": predict_CO_class_using_naive_baseline
}

params_grids_by_clf_name = {
    "Decision Tree": {
        "max_depth": [1, 3, 5, 7, 9, 11, 13, 15, None],
        "min_samples_leaf": [1, 5, 10, 15, 20, 25],
        "max_features": [None, "sqrt", "log2"],
    },
    "Support Vector Machine": {
        "C": [0.1, 1, 10, 100, 1000],
        "gamma": ["scale", 0.1, 0.01, 0.001, 0.0001],
    },
    "Logistic Regression": {
        "C": [0.01, 0.1, 1, 10, 100, 1000]
    },
    "Naive Baseline": {}

}

results = pd.DataFrame({
    "Horizon": horizons,
    "Decision Tree accuracy": [0] * len(horizons),
    "Support Vector Machine accuracy": [0] * len(horizons),
    "Logistic Regression accuracy": [0] * len(horizons),
    "Naive Baseline accuracy": [0] * len(horizons)
})

for h in horizons:
    tuning_train_ratio = 0.8
    tuning_train_test_split = int(len(train) * tuning_train_ratio)
    train_train = train.iloc[:tuning_train_test_split]
    train_test = train.iloc[tuning_train_test_split:]
    y_train_test = train_test[f"CO_fut_{h}h_class_l_encoded"]

    eff_test = test.iloc[:(-1 * h)]
    y_test = eff_test[f"CO_fut_{h}h_class_l_encoded"]

    for clf_name, predict_CO_class in predictor_by_clf_name.items():
        # Hyperparameters tuning
        params_grids = params_grids_by_clf_name[clf_name]
        param_keys = list(params_grids.keys())
        if len(param_keys) > 0:
            params_list = [params_grids[param_keys[0]]]
        else:
            param_list = []
        for i in range(1, len(param_keys)):
            result = []
            next_list = params_grids[param_keys[i]]
            for i in range(0, len(params_list)):
                for j in range(0, len(next_list)):
                    if type(params_list[i]) != list:
                        params_list[i] = [params_list[i]]
                    temp = [num for num in params_list[i]]
                    temp.append(next_list[j])
                    result.append(temp)
            params_list = result
        params_list = [dict(zip(param_keys, row_list)) for row_list in params_list]

        best_params = None
        best_tuning_acc = -np.inf

        print(f"Tuning hyperparameters of {clf_name} for horizon {h}...")
        for curr_params in params_list:
            print("curr_params:", curr_params)
            y_pred_train_test = predict_CO_class(curr_params, train_train, train_test, h)
            curr_tuning_acc = accuracy_score(y_train_test,y_pred_train_test)
            print(clf_name, "with params:", curr_params, f"=> accuracy = {curr_tuning_acc:.4f}")

            if curr_tuning_acc > best_tuning_acc:
                best_tuning_acc = curr_tuning_acc
                best_params = curr_params

        print(f"For horizon: {h}, best params on validation:", best_params, f"with accuracy: {best_tuning_acc:.4f}")

        # Train and test the model with the tuned hyperparameters
        print(f"{clf_name} model training for horizon: {h}...")
        y_pred_test = predict_CO_class(best_params, train, eff_test, h)
        test_acc = accuracy_score(y_test, y_pred_test)

        print(f"Finished using {clf_name} model for horizon: {h}! Accuracy: {test_acc:.4f}\n\n")

        results.loc[results["Horizon"] == h, clf_name + " accuracy"] = test_acc

results

"""# Accuracy Plot"""

ax = results.plot(kind = "line", x = "Horizon", figsize=(8, 5))
ax.set_xlabel('Horizon (t + x hours)')
ax.set_ylabel('Accuracy')
ax.set_title('Classification Accuracy by Horizon')
plt.savefig("classification-accuracy.png")
plt.show()
plt.close()

"""# Export Report"""

print("\n=== Exporting Data ===")

# Export to CSV
try:
    csv_filename = "co_class_results.csv"
    results.to_csv(csv_filename, index = False)
    print(f"Successfully saved results to '{csv_filename}'")
except Exception as e:
    print(f"Error saving CSV: {e}")

# Export to Excel
try:
    xlsx_filename = "co_class_results.xlsx"
    results.to_excel(xlsx_filename, index = False)
    print(f"Successfully saved results to '{xlsx_filename}'")
except ImportError:
    print(f"Could not save to Excel ({xlsx_filename}). Missing dependency.\nPlease run: pip install openpyxl")
except Exception as e:
    print(f"Error saving Excel: {e}")