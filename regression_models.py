# -*- coding: utf-8 -*-
"""regression_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WyJpWU2B9XQmrxEMSXuL3snR4jcTF9eh

**Importing the relevant libraries**
"""

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler

"""**Setup**

set plotting style, loading the file and initialise variables
"""

# Set plotting style
plt.style.use('ggplot')

# Load data
try:
    df = pd.read_csv("air_quality_clean.csv", index_col="Datetime", parse_dates=True)
except FileNotFoundError:
    # Create dummy data if file doesn't exist for demonstration
    dates = pd.date_range(start="2004-01-01", end="2005-12-31", freq="H")
    df = pd.DataFrame(np.random.rand(len(dates), 5) * 100, index=dates, columns=["CO(GT)", "NMHC(GT)", "C6H6(GT)", "NOx(GT)", "NO2(GT)"])

# Pollutants to model
pollutants = ["CO(GT)", "NMHC(GT)", "C6H6(GT)", "NOx(GT)", "NO2(GT)"]
horizons = [1, 6, 12, 24]
lags = [1, 2, 3, 24]

all_results = []

"""## Time Series Forecasting Model Training and Evaluation

This section trains and evaluates multiple machine learning models (Linear Regression, Random Forest, XGBoost, Decision Tree, Neural Network, and Support Vector Machine) for time series forecasting. It iterates through different pollutants and prediction horizons, calculates performance metrics (RMSE), and generates visualizations for the first prediction horizon to compare actual vs. predicted values and analyze residuals.
"""

for pollutant in pollutants:
    print(f"\n{'='*10} Modelling pollutant: {pollutant} {'='*10}")

    df_temp = df.copy()

    # 1. Create Lag Features
    for lag in lags:
        df_temp[f"{pollutant}_lag_{lag}"] = df_temp[pollutant].shift(lag)

    # 2. Create targets
    for h in horizons:
        df_temp[f"y_{h}"] = df_temp[pollutant].shift(-h)

    # 3. Train/test split
    # We need to be careful to drop NaNs created by shifting
    train = df_temp[df_temp.index.year == 2004].dropna()
    test = df_temp[df_temp.index.year == 2005].dropna()

    # 4. Prepare Features
    # We keep 'pollutant' (current time t) in 'test' temporarily for Baseline calculation
    drop_cols_X = [pollutant] + [f"y_{h}" for h in horizons]

    X_train = train.drop(columns=drop_cols_X)
    X_test = test.drop(columns=drop_cols_X)

    # 5. Scale Data
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    for h in horizons:
        y_train = train[f"y_{h}"]
        y_test = test[f"y_{h}"]

        print(f"\n=== Model training for horizon: {h} ===")

        # --- Naive Baseline ---
        # Prediction for t+h is simply the value at t
        # In our dataframe, test[pollutant] is the value at t
        baseline_pred = test[pollutant]
        rmse_baseline = np.sqrt(mean_squared_error(y_test, baseline_pred))
        print(f"Baseline (Naïve) RMSE: {rmse_baseline:.4f}")

        # --- Linear Regression ---
        print("Linear Regression model training...")
        start_time = time.time()
        lr = LinearRegression()
        lr.fit(X_train_scaled, y_train)
        pred_lr = lr.predict(X_test_scaled)
        rmse_lr = np.sqrt(mean_squared_error(y_test, pred_lr))
        print(f"Finished Linear Regression! Time: {time.time() - start_time:.2f}s, RMSE: {rmse_lr:.4f}")

        # --- Random Forest ---
        print("Random Forest model training...")
        start_time = time.time()
        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        rf.fit(X_train, y_train)
        pred_rf = rf.predict(X_test)
        rmse_rf = np.sqrt(mean_squared_error(y_test, pred_rf))
        print(f"Finished Random Forest! Time: {time.time() - start_time:.2f}s, RMSE: {rmse_rf:.4f}")

        # --- XGBoost ---
        print("XGBoost model training...")
        start_time = time.time()
        xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42, n_jobs=-1)
        xgb.fit(X_train, y_train)
        pred_xgb = xgb.predict(X_test)
        rmse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))
        print(f"Finished XGBoost! Time: {time.time() - start_time:.2f}s, RMSE: {rmse_xgb:.4f}")

        # --- Decision Tree ---
        print("Decision Tree model training...")
        start_time = time.time()
        dt = DecisionTreeRegressor(max_depth=10, random_state=42)
        dt.fit(X_train, y_train)
        pred_dt = dt.predict(X_test)
        rmse_dt = np.sqrt(mean_squared_error(y_test, pred_dt))
        print(f"Finished Decision Tree! Time: {time.time() - start_time:.2f}s, RMSE: {rmse_dt:.4f}")

        # --- Neural Network ---
        print("Neural Network (MLP) model training...")
        start_time = time.time()
        nn = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=500, random_state=42, early_stopping=True)
        nn.fit(X_train_scaled, y_train)
        pred_nn = nn.predict(X_test_scaled)
        rmse_nn = np.sqrt(mean_squared_error(y_test, pred_nn))
        print(f"Finished Neural Network! Time: {time.time() - start_time:.2f}s, RMSE: {rmse_nn:.4f}")

        # --- SVM ---
        print("Support Vector Machine (SVR) model training...")
        start_time = time.time()
        svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)
        svr.fit(X_train_scaled, y_train)
        pred_svr = svr.predict(X_test_scaled)
        rmse_svr = np.sqrt(mean_squared_error(y_test, pred_svr))
        print(f"Finished SVR! Time: {time.time() - start_time:.2f}s, RMSE: {rmse_svr:.4f}")

        all_results.append([pollutant, h, rmse_baseline, rmse_lr, rmse_rf, rmse_xgb, rmse_dt, rmse_svr, rmse_nn])

        # --- Visualisation (Only for Horizon = 1 to save space/time) ---
        if h == 1:
            # 1. Trend Plot (Actual vs Predicted - First 150 hours for clarity)
            plt.figure(figsize=(12, 5))
            subset_n = 150
            plt.plot(y_test.index[:subset_n], y_test.values[:subset_n], label='Actual', color='black', alpha=0.7)
            plt.plot(y_test.index[:subset_n], pred_rf[:subset_n], label='Random Forest', color='blue', alpha=0.7)
            plt.plot(y_test.index[:subset_n], pred_lr[:subset_n], label='Linear Reg', color='green', linestyle='--', alpha=0.7)
            plt.title(f'Forecast Trends: {pollutant} (Horizon h={h})')
            plt.xlabel('Date')
            plt.ylabel('Concentration')
            plt.legend()
            plt.savefig(f'trend_{pollutant}_h{h}.png')
            plt.close()

            # 2. Residual Plot (Random Forest Residuals)
            residuals = y_test - pred_rf
            plt.figure(figsize=(12, 5))
            plt.scatter(y_test, residuals, alpha=0.3, color='red')
            plt.axhline(0, color='black', linestyle='--')
            plt.title(f'Residual Plot (Random Forest): {pollutant} (Horizon h={h})')
            plt.xlabel('Actual Values')
            plt.ylabel('Residuals (Actual - Predicted)')
            plt.savefig(f'residuals_{pollutant}_h{h}.png')
            plt.close()


print("Model Training and Evaluation finished")

"""**Convert to DataFrame**"""

df_all_results = pd.DataFrame(
    all_results,
    columns=[
        'Pollutant',
        'Horizon',
        'Baseline RMSE',
        'Linear Regression RMSE',
        'Random Forest RMSE',
        'XGBoost RMSE',
        'Decision Tree RMSE',
        'SVM RMSE',
        'Neural Network RMSE'
    ]
)

"""**Prints formatted tables**"""

print("\n" + "="*30)
print(" FINAL RESULTS TABLES ")
print("="*30)

for pollutant in pollutants:
    print(f"\nResults for {pollutant}:")
    display_df = df_all_results[df_all_results['Pollutant'] == pollutant].drop(columns=['Pollutant'])
    display_df = display_df.reset_index(drop=True)
    print(display_df.to_string())
    print("-" * 100)

"""**Export results**"""

print("\n=== Exporting Data ===")

# Export to CSV
try:
    csv_filename = "air_quality_results.csv"
    df_all_results.to_csv(csv_filename, index=False)
    print(f"Successfully saved results to '{csv_filename}'")
except Exception as e:
    print(f"Error saving CSV: {e}")

# Export to Excel
try:
    xlsx_filename = "air_quality_results.xlsx"
    df_all_results.to_excel(xlsx_filename, index=False)
    print(f"Successfully saved results to '{xlsx_filename}'")
except ImportError:
    print(f"Could not save to Excel ({xlsx_filename}). Missing dependency.\nPlease run: pip install openpyxl")
except Exception as e:
    print(f"Error saving Excel: {e}")

"""## Observation and Results Summary

Based on the results from the model training and evaluation, here's a summary of the conclusions:

1.  **Significant Improvement Over Baseline**: All implemented machine learning models (Linear Regression, Random Forest, XGBoost, Decision Tree, Neural Network, and SVM) consistently achieved significantly lower RMSE values compared to the Baseline (Naïve) model across all pollutants and prediction horizons. This indicates that even simple machine learning approaches are much more effective than a naive forecast for this dataset.

2.  **Top Performing Models**: Linear Regression, Support Vector Machine (SVM), and Neural Network (MLP) models generally showed the best performance, often yielding the lowest RMSE values for most pollutants and horizons.

3.  **Intermediate Performers**: Random Forest and XGBoost models performed well, consistently outperforming the Baseline and Decision Tree, but were often slightly behind Linear Regression, SVM, and Neural Networks in terms of RMSE.

4.  **Decision Tree Performance**: The Decision Tree Regressor consistently had higher RMSE values compared to other machine learning models, suggesting it was the least effective among the advanced models for this forecasting task.

5.  **Consistency Across Horizons**: The relative performance rankings of the models remained fairly consistent across different prediction horizons (1, 6, 12, and 24 hours). There wasn't a single model that drastically improved or worsened its relative standing at longer horizons.

In essence, the advanced models provide robust improvements over simple baseline methods, with Linear Regression, SVM, and Neural Networks showing the strongest predictive power for this time series forecasting problem.
"""